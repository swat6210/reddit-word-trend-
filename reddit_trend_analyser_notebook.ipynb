{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356eb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df0d3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec2fb484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: textblob in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (8.2.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install praw\n",
    "%pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b00cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id='CMGf7igMOFt-KRPrK8TuaQ',\n",
    "    client_secret='P6nKUm0hO3fHIEBoN4YuF42ZYTsR0g',\n",
    "    user_agent='trend analyser'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03fa4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_posts_by_topic(topic, limit=100):\n",
    "    subreddit = reddit.subreddit(\"all\")\n",
    "    posts = []\n",
    "    for submission in subreddit.search(topic, limit=limit):\n",
    "        posts.append({\n",
    "            \"title\": submission.title,\n",
    "            \"body\": submission.selftext\n",
    "        })\n",
    "    return posts\n",
    "\n",
    "def extract_trending_words(posts, top_n=15):\n",
    "    all_text = \" \".join(p[\"title\"] + \" \" + p[\"body\"] for p in posts).lower()\n",
    "    tokens = word_tokenize(all_text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    counter = Counter(filtered)\n",
    "    return counter.most_common(top_n)\n",
    "\n",
    "def analyze_sentiment(posts):\n",
    "    pos = neu = neg = 0\n",
    "    for post in posts:\n",
    "        text = post[\"title\"] + \" \" + post[\"body\"]\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        if polarity > 0.1:\n",
    "            pos += 1\n",
    "        elif polarity < -0.1:\n",
    "            neg += 1\n",
    "        else:\n",
    "            neu += 1\n",
    "    return {\"positive\": pos, \"neutral\": neu, \"negative\": neg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b274a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_topic(topic: str, limit=100):\n",
    "    posts = search_posts_by_topic(topic, limit)\n",
    "    top_words = extract_trending_words(posts)\n",
    "    sentiment = analyze_sentiment(posts)\n",
    "    return {\n",
    "        \"topic\": topic,\n",
    "        \"top_words\": [{\"word\": w, \"count\": c} for w, c in top_words],\n",
    "        \"sentiment\": sentiment,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f065553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis for topic: genshinimpact\n",
      "\n",
      "Top Trending Words:\n",
      "would: 17\n",
      "even: 16\n",
      "know: 15\n",
      "characters: 15\n",
      "traveller: 13\n",
      "like: 13\n",
      "one: 10\n",
      "character: 9\n",
      "va: 9\n",
      "still: 9\n",
      "design: 8\n",
      "pages: 8\n",
      "could: 7\n",
      "genshin: 7\n",
      "game: 6\n",
      "\n",
      "Sentiment Distribution:\n",
      "{'positive': 17, 'neutral': 29, 'negative': 4}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    topic = \"genshinimpact\"\n",
    "    result = analyze_topic(topic, limit=50)\n",
    "    \n",
    "    print(f\"Analysis for topic: {topic}\")\n",
    "    print(\"\\nTop Trending Words:\")\n",
    "    for entry in result[\"top_words\"]:\n",
    "        print(f\"{entry['word']}: {entry['count']}\")\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(result[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80329c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
